---
title: "Performance, Reliability, and Troubleshooting"
description: "Best practices for increasing speed, handling rate limits, debugging agent behaviors, managing failures in tool/API calls, and improving structured output reliability. A practical 'what works' guide for smooth operations."
---

# Performance, Reliability, and Troubleshooting

This guide provides practical advice on optimizing Open Deep Research agent workflows for speed, reliability, and smooth execution. It covers best practices for managing concurrency, handling rate limits, debugging common issues, mitigating API/tool call failures, and ensuring consistent structured output from models.

---

## 1. Optimizing Performance and Speed

### Understanding the Research Workflow

Open Deep Research uses asynchronous, parallel research units managed by supervisor and researcher agents. Researcher agents perform tool calls concurrently up to configured limitsâ€”correct tuning of these parameters greatly impacts speed.

### Key Configuration Parameters

- **`max_concurrent_research_units`**: Controls parallelism of research tasks. Increasing this value accelerates research but can trigger rate limits on APIs.
- **`max_researcher_iterations`**: Maximum supervisor reflection cycles. Higher values increase research depth but lengthen runtime.
- **`max_react_tool_calls`**: Max tool calls per researcher message. Controls granularity of research steps.

### Best Practices

- Start with default concurrency (usually 5) and gradually increase based on your API quota and responsiveness.
- Use efficient search APIs like Tavily or DuckDuckGo for faster responses.
- Opt for lighter, faster models for synthesis steps (e.g., compression) to speed up report generation.
- Adjust model max tokens according to expected output size to avoid token clipping and retries.

### Example: Faster Research Run

```json
{
  "configurable": {
    "max_concurrent_research_units": 10,
    "max_researcher_iterations": 3,
    "search_api": "tavily",
    "research_model": "openai:gpt-4.1",
    "compression_model": "openai:gpt-4.1-mini"
  }
}
```

---

## 2. Handling Rate Limits and API Quotas

### Common Scenarios

- Hitting limits with external search APIs or large model usage
- API errors (e.g., HTTP 429 Too Many Requests)

### Mitigation Strategies

- Lower `max_concurrent_research_units` to reduce simultaneous requests.
- Increase retry attempts with exponential backoff in `max_structured_output_retries`.
- Use caching layers (LangGraph memory saver) to avoid redundant calls.
- Monitor API usage actively via dashboard or logs.
- For high-volume use, consider paid API plans with higher quotas.

### Configuration Tip

Add these to the `.env` or runtime config:

```ini
# Increased retries
MAX_STRUCTURED_OUTPUT_RETRIES=5

# Lower concurrency during heavy usage
MAX_CONCURRENT_RESEARCH_UNITS=3
```

---

## 3. Debugging Agent Behaviors and Workflow Failures

### Common Issues

- Agent stalls or hangs during research
- Unexpected token limit exceeded errors
- Incomplete or missing tool call outputs

### Debugging Steps

1. **Enable Verbose Logging**: Run LangGraph server in debug mode:

```bash
langgraph dev --log-level debug
```

2. **Inspect Agent States**: Use LangGraph Studio to view `supervisor_messages` and `researcher_messages` queues and tool calls.

3. **Check Model Responses**:
- Look for repeated retries or missing structured outputs.
- Note any malformed or partial JSON outputs.

4. **Validate Tool Configurations**:
- Confirm tools are accessible and correctly bound.
- Check MCP server URLs, command line args, and environment variables.

5. **Simulate Single Steps**:
- Invoke functions separately in a local notebook or script to isolate failures.

---

## 4. Managing Failures in Tool and API Calls

### Failure Scenarios

- Network timeouts
- API rate limit exceeded
- Tool runtime errors

### Automatic Handling

The system automatically:
- Retries tool calls and model outputs (configurable retry counts)
- Prunes conversation history to stay within token limits
- Captures error messages in tool response content

### User Actions

- Increase retry counts if frequent failures occur.
- Reduce concurrency to avoid floods.
- Catch and review tool error messages within final report.

### Example: Capturing and Logging Tool Errors

If a tool fails, the agent produces a `ToolMessage` with error details. Use this info to debug or provide fallback data.

```python
async def execute_tool_safely(tool, args, config):
    try:
        return await tool.ainvoke(args, config)
    except Exception as e:
        return f"Error executing tool: {str(e)}"
```

---

## 5. Improving Structured Output Reliability

### Why It Matters

Models provide structured outputs to drive control flow and data transfer between agents. Parsing errors or incomplete output can stall the workflow.

### Strategies to Improve

- Use models that natively support structured output (e.g., OpenAI GPT-4 series, Anthropic Claude 3.7+).
- Set `max_structured_output_retries` to at least 3 to automatically retry failed parses.
- Design prompts and output schemas clearly to minimize ambiguity.
- Use explicit stop sequences and function calling when available.

### Avoid Common Pitfalls

- Do not use models without structured output support.
- Avoid extremely large context windows that may cause truncations.
- Prefer smaller, focused prompts for critical structured output steps.

### Example: Configuring Model for Structured Output

```python
config = {
    "configurable": {
        "research_model": "openai:gpt-4.1",
        "max_structured_output_retries": 5,
    }
}
```

---

## 6. Troubleshooting Checklist

| Issue                                | Possible Cause                       | Solution                                             |
|------------------------------------|------------------------------------|------------------------------------------------------|
| Agent hangs/stalls                 | Token limit exceeded, infinite loop | Reduce prompt size, increase retries, prune history  |
| No tool outputs                   | Tool binding missing or misconfigured | Verify tool setup, validate tool calls in logs       |
| Final report incomplete or empty  | Early exit due to token limits or errors | Review retry counts, check error messages            |
| Rate limit errors (HTTP 429)      | Too many concurrent API calls       | Lower concurrency, add delays, upgrade API tier      |
| Parsing errors on structured output | Model output format unexpected      | Increase retries, clarify prompts, switch models     |
| MCP server connection failure     | Server offline or misconfigured     | Check server status, transport method, creds         |

---

## 7. Practical Tips and Best Practices

- **Monitor Logs Regularly**: Early detection of rate limits or errors helps maintain uptime.
- **Use Checkpointing**: Enable LangGraph memory saver to checkpoint states and ease recovery.
- **Scale Gradually**: Increase concurrency and iteration counts incrementally.
- **Leverage MCP Tools**: For richer context, incorporate MCP servers to offload API calls.
- **Test Tools Separately**: Always validate new tools or APIs outside the research workflow first.


## 8. Next Steps

After mastering performance tuning and troubleshooting, consider exploring:

- [Configuring Models, Search, and Research Parameters](../core-workflows/configuring-research-agents)
- [Clarification, Reflection, and Advanced Iterations](../core-workflows/clarification-and-iteration)
- [Integrating Search APIs and MCP Servers](../integration-patterns/using-search-mcp-models)
- [Evaluating and Benchmarking Research Output](../evaluation-and-best-practices/running-evaluations)

---

## Appendix: Example Configuration Snippet

```json
{
  "configurable": {
    "max_structured_output_retries": 3,
    "allow_clarification": false,
    "max_concurrent_research_units": 5,
    "search_api": "tavily",
    "max_researcher_iterations": 3,
    "max_react_tool_calls": 5,
    "summarization_model": "openai:gpt-4.1-nano",
    "summarization_model_max_tokens": 8192,
    "research_model": "openai:gpt-4.1",
    "research_model_max_tokens": 10000,
    "compression_model": "openai:gpt-4.1-mini",
    "compression_model_max_tokens": 10000,
    "final_report_model": "openai:gpt-4.1",
    "final_report_model_max_tokens": 10000
  }
}
```

---

## Glossary

- **Agent**: An autonomous sub-component performing research or supervision
- **Tool Call**: A request issued by an agent to an external API or service
- **Structured Output**: Machine-readable, validated responses from LLMs
- **Concurrency**: Number of parallel tasks running simultaneously
- **MCP (Model Context Protocol)**: Open standard for connecting LLMs to external data and tools

---

For more detailed technical insights, see source files:

- [`src/open_deep_research/deep_researcher.py`](https://github.com/langchain-ai/open_deep_research/blob/main/src/open_deep_research/deep_researcher.py)
- [`src/open_deep_research/state.py`](https://github.com/langchain-ai/open_deep_research/blob/main/src/open_deep_research/state.py)

---